Cloudera开发的Cloudera Manager大大简化了Hadoop系统作业的管理和跟踪工作
可以安装Cloudera 5虚拟机，执行操作步骤后开发环境最终会安装成功，快速搭建Hadoop开发环境

参考：http://www.aboutyun.com/thread-23773-1-1.html

1.安装系统
Hadoop一般在Linux系统下搭建和使用，Windows系统存在许多问题
    1.1安装虚拟机
        使用VMware Workstation 14 Player，免费
        其他可选软件：VMware Workstation 14 Pro，收费；Oracle VM VirtualBox，免费
    
    1.2安装操作系统
        物理机母系统：Win10
        物理机子系统：Ubuntu
            安装后根目录：
            C:\Users\win10用户名\AppData\Local\Packages\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\LocalState\rootfs\home\Linux新建的用户名
        虚拟机系统：Ubuntu server 16.04.4 LTS
        都需要完成完成Linux系统下的基本操作
            $ sudo apt-get update
            $ sudo apt-get upgrade
        
    1.3配置IP
        虚拟机系统安装完成后，需要修改为静态IP地址：
            $ sudo vim /etc/network/interfaces
        原内容有如下4行：
            auto lo
            iface lo inet loopback

            auto eth33
            iface eth0 inet dhcp
            // 以上表示默认使用DHCP分配IP，以一台虚拟机为例，修改如下：
            auto lo
            iface lo inet loopback

            # The primary network interface
            auto eth33
            #iface eth33 inet dhcp

            iface eth33 inet static
            address 192.168.56.130
            netmask 255.255.255.0
            gateway 192.168.56.2

    1.4设置DNS服务器
        输入命令：
            $ sudo vim /etc/resolv.conf
        添加内容:
            nameserver 192.168.56.2
        虚拟机不使用时挂起，一般不关机，以下内容备用：
            重启Ubuntu后发现不能上网，问题出在/etc/resolv.conf
            重启后此文件配置的DNS又被自动修改为默认值，需要永久性修改DNS：
                $ sudo vim /etc/resolvconf/resolv.conf.d/base
            添加内容：
                nameserver 192.168.80.2
            重启networking服务使其生效：
                $ sudo /etc/init.d/networking restart

2.安装必要软件
    2.1ssh配置
        安装SSH-server、SSH-client：
            $ sudo apt-get install openssh-server
            $ sudo apt-get install openssh-client
        查看SSH是否启动，若没有任何显示则未安装/启动该服务：
            $ ps -e | grep ssh
        启动SSH（停止用stop，重启用restart），Win10子系统默认不启动ssh服务，需要手动开启：
            $ sudo /etc/init.d/ssh start  # 通过openssh服务器的启动脚本文件启动
            $ sudo service ssh start  # 通过Linux下的service命令启动，效果同上
        开启root登录：
            $ sudo vim /etc/ssh/sshd_config
            PermitRootLogin yes
        自动接受新的公钥：
            $ sudo vim /etc/ssh/ssh_config
            StrictHostKeyChecking no  # ssh、scp等实现当第一次连接服务器时自动接受新公钥
        修改默认端口：
            ssh默认端口为22，Windows系统已为自己的CMD应用将其占用，Win10子系统可访问虚拟机，虚拟机无法反向访问
            $ sudo vim /etc/ssh/sshd_config  # 所有workers都需要修改
            Port 23
            Win10开了防火墙，需要修改防火墙设置，建立入站规则，允许传入的连接申请（allow incoming traffic）到23号端口，以便允许远程登陆
        修改hosts，Win10子系统需要修改母系统的hosts，子系统自身的修改无效：
            $ sudo vim /etc/hosts
            # hadoop nodes
            192.168.56.1   ubuntu-1
            192.168.56.130 ubuntu-2
            192.168.56.131 ubuntu-3
        配置免密登陆：
            $ ssh-keygen -t rsa  # 产生密钥
            $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys  # 导入authorized_keys
            $ scp -P 23 ~/.ssh/authorized_keys 用户名1@hostname1:~/.ssh  # 分发
            $ scp -P 23 ~/.ssh/authorized_keys 用户名2@hostname2:~/.ssh
        重启openssh-server：
            $ sudo service ssh restart
        测试：
            $ ssh hostname1
        备用内容：
            启动ssh-agent：
            $ sudo eval ssh-agent

    2.2Java环境
        创建新的文件夹用于存放JDK，或直接存放默认目录
        在线下载JDK，注意http地址是下载页面里对应JDK文件的链接地址，分发：
            $ sudo curl -o jdk-10_linux-x64_bin.tar.gz -L --cookie "oraclelicense=accept-securebackup-cookie"  http://download.oracle.com/otn-pub/java/jdk/10+46/76eac37278c24557a3c4199677f19b62/jdk-10_linux-x64_bin.tar.gz
            $ sudo scp -P 23 jdk-10_linux-x64_bin.tar.gz 用户名1@hostname1:
            $ sudo scp -P 23 jdk-10_linux-x64_bin.tar.gz 用户名2@hostname2:
        解压，删除压缩包：
            $ sudo tar -zxvf jdk-10_linux-x64_bin.tar.gz
            $ sudo rm jdk-10_linux-x64_bin.tar.gz
        修改环境变量
            $ sudo vim ~/.bashrc
            // 文件末尾追加:
            #Set oracle jdk environment
            export JAVA_HOME=/home/用户名/jdk-10
            export JRE_HOME=${JAVA_HOME}/jre
            export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
            export PATH=${JAVA_HOME}/bin:$PATH
        使环境变量马上生效，分发：
            $ sudo source ~/.bashrc
            $ sudo scp -P 23 ~/.bashrc 用户名1@hostname1:~/
            $ sudo scp -P 23 ~/.bashrc 用户名2@hostname2:~/
        验证JDK是否安装配置成功
            $ java -version

3.Hadoop安装及调试
    3.1下载解压
        创建新的文件夹用于存放Hadoop，或直接存放默认目录
            $ sudo curl -o hadoop-3.0.1.tar.gz -L http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.0.1/hadoop-3.0.1.tar.gz
        解压，删除压缩包：
            $ sudo tar -zxvf hadoop-3.0.1.tar.gz
            $ sudo rm hadoop-3.0.1.tar.gz
        环境配置：
            $ sudo vim ~/.bashrc
            # Set Hadoop environment
            export HADOOP_HOME=/home/用户名/hadoop-3.0.0
            export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

            $ sudo vim hadoop-3.0.0/etc/hadoop/hadoop-env.sh
            export JAVA_HOME=/home/用户名/jdk-10
            export HADOOP_SSH_OPTS="-p 23"  # 因为ssh修改了端口

            $ sudo vim hadoop-3.0.0/etc/hadoop/yarn-env.sh
            export JAVA_HOME=/home/用户名/jdk-10

            $ sudo vim hadoop-3.0.0/etc/hadoop/workers
            192.168.56.1
            192.168.56.130  # 或者hostname1
            192.168.56.131  # 或者hostname2
    
    3.2修改配置文件
        $ sudo vim hadoop-3.0.0/etc/hadoop/core-site.xml
        <configuration>
          <property>
            <name>fs.defaultFS</name>  # 默认文件系统的结构、主机名、端口号：使用HDFS文件系统，名称节点运行主机，使用9000端口
            <!-- 或hosts配置的域名的对应主机名，如master -->
            <value>hdfs://192.168.56.1:9000/</value>
          </property>
          <property>
            <name>hadoop.tmp.dir</name>  # 临时路径
            <value>/home/用户名/hadoop-3.0.0/tmp</value>  # 手动创建对应目录：$ sudo mkdir /home/用户名/hadoop-3.0.0/tmp
          </property>
        </configuration>

        $ sudo vim hadoop-3.0.0/etc/hadoop/hdfs-site.xml
        <configuration>
          <property>
            <name>dfs.namenode.name.dir</name>
            <value>/home/用户名/hadoop-3.0.0/dfs/namenode</value>  # 手动创建对应目录：$ sudo mkdir /home/用户名/hadoop-3.0.0/dfs/namenode
          </property>
          <property>
            <name>dfs.datanode.data.dir</name>
            <value>/home/用户名/hadoop-3.0.0/dfs/datanode</value>  # 手动创建对应目录：$ sudo mkdir /home/用户名/hadoop-3.0.0/dfs/datanode
          </property>
          <property>
            <name>dfs.replication</name>  # 副本数目
            <value>1</value>
          </property>
          <property>
            <name>dfs.permissions.enabled</name>  # true，则在HDFS中启用权限检查；false，权限检查被关闭，缺少此配置可能在远程开发中会连接失败
            <value>false</value>
          </property>
        </configuration>

        $ sudo vim hadoop-3.0.0/etc/hadoop/mapred-site.xml
        <configuration>
          <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>  # local表示本地运行，classic表示经典mapreduce框架，yarn表示新框架
          </property>
          <property>
            <name>mapreduce.admin.user.env</name>
            <value>HADOOP_MAPRED_HOME=/home/用户名/hadoop-3.0.0</value>
            // 如果map和reduce任务访问本地库（压缩等），必须保留原始值。
            // 当此值为空时，设置执行环境的命令将取决于操作系统：
            // Linux：LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native.
            // Windows：PATH =%PATH%;%HADOOP_COMMON_HOME%\\bin.
          </property>
          <property>
            <name>yarn.app.mapreduce.am.env</name>  # 可设置AM（AppMaster）端的环境变量
            <value>HADOOP_MAPRED_HOME=/home/用户名/hadoop-3.0.0</value>
          </property>
        </configuration>

        $ sudo vim hadoop-3.0.0/etc/hadoop/yarn-site.xml
        <configuration>
        <!-- Site specific YARN configuration properties -->
          <property>
            <name>yarn.nodemanager.aux-services</name>  # NodeManager上运行的附属服务
            <value>mapreduce_shuffle</value>
          </property>
          <property>                                                             
            <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
          </property>
        </configuration>

        分发:
            $ scp -P 23 -r /home/用户名/hadoop-3.0.0 用户名1@hostname1:/home/用户名
            $ scp -P 23 -r /home/用户名/hadoop-3.0.0 用户名2@hostname2:/home/用户名
